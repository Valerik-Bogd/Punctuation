{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valerik-Bogd/Punctuation/blob/main/Prepare_datasets_distilled_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# name of the general folder\n",
        "data_path = '/content/drive/MyDrive/Data_punct'\n",
        "\n",
        "# raw_data_path = '/content/drive/MyDrive/Data_punct/Raw_data'\n",
        "# out_path = '/content/drive/MyDrive/Data_punct/Datasets'\n",
        "\n",
        "# name of folder to prepare, which lies in raw_data_path:\n",
        "data_to_conv = 'Legal_docx'\n",
        "files_ext = 'txt'"
      ],
      "metadata": {
        "id": "L_DfXhszGH99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_conv_path = f'{data_path}/Raw_data/{data_to_conv}'"
      ],
      "metadata": {
        "id": "0UvwIL7WGH9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "!pip install pypandoc\n",
        "import pypandoc\n",
        "\n",
        "!pip install docx2txt\n",
        "import docx2txt\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf7f403-6e5b-452b-d56d-a380c3abc46e",
        "id": "Pm_7TSrsGH9-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.12-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.12\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=2d86aa89f3449cddef2441f32fb3047f70b0bece89a4b981d079fc6a8d8d43cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKvqgdIhGH6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Files_Prep:\n",
        "    def __init__(self, data_to_conv, files_ext, data_path='/content/drive/MyDrive/Data_punct'):\n",
        "        self.data_to_conv_path = f'{data_path}/Raw_data/{data_to_conv}'\n",
        "        self.files_ext = files_ext\n",
        "        self.delete_list = [\" Документ предоставлен \", \" КонсультантПлюс: \", \" КонсультантПлюс \",\n",
        "                            \" [] \", \" []\\n\", \"\\n[] \", \"\\r[]\\r\", \"+-\", \" -- \", \" -+\", \"+=\",\"=+\",\n",
        "                            \" = \", \" | \", \" + \", \" -\", \"\\r\\n []\", \"\\n []\", \"\\n\\n\", \"\\n[]\\n\", \"$[]$\"]\n",
        "\n",
        "        self.all_punct = [\"'\", '\"',\".\", \",\", \"!\", \"?\",\"-\", \":\", \";\"]\n",
        "        self.names_punct = ['S_QUOTE', 'D_QUOTE',\n",
        "                            'PERIOD', 'COMMA', 'EX_MARK', 'Q_MARK',\n",
        "                            'DASH', 'COLUMN', 'SEMI_COL'\n",
        "                            ]\n",
        "        self.punct_marks = dict(zip(self.all_punct, self.names_punct))\n",
        "\n",
        "        self.files = []\n",
        "        # r=root, d=directories, f = files\n",
        "        for r, d, f in os.walk(self.data_to_conv_path):\n",
        "            for file in f:\n",
        "                if file.endswith((f'.{files_ext}')):\n",
        "                    self.files.append(os.path.join(r, file))\n",
        "        print(self.files)\n",
        "\n",
        "    def process_files(self):\n",
        "        print(self.files_ext)\n",
        "        # files = os.listdir(self.data_to_conv_path)\n",
        "\n",
        "        if self.files_ext not in ('txt', 'csv'):\n",
        "            self.convert_to_txt(self.files)\n",
        "\n",
        "        if self.files_ext == 'txt':\n",
        "            self.clean_split_txt(self.files)\n",
        "        elif self.files_ext == 'csv':\n",
        "            self.clean_split_csv(self.files)\n",
        "\n",
        "        # now files ought to be .txt paragraphs in dataset folder\n",
        "        self.make_to_dataset(f'{data_path}/Datasets/{data_to_conv}')\n",
        "\n",
        "\n",
        "    def convert_to_txt(self, file_path):\n",
        "        # Convert the file to txt format\n",
        "        if self.files_ext not in ('doc', 'docx'):\n",
        "            print('not ready')\n",
        "            return\n",
        "\n",
        "        elif self.files_ext in ('doc', 'docx'):\n",
        "            self.txt_files = []\n",
        "            for doc_file_name in self.files:\n",
        "                file_name = doc_file_name.split(\"/\")[-1]\n",
        "                txtFilename = data_path + \"/Raw_data/\" + data_to_conv + \"/\" + file_name.split(\".\")[0] + \".txt\"\n",
        "                self.txt_files.append(txtFilename)\n",
        "                print ('files will be saved as:', txtFilename)\n",
        "                #output = pypandoc.convert_file(docxFilename, 'plain', txtFilename)\n",
        "                output = pypandoc.convert_file(doc_file_name, 'plain',extra_args=('--standalone','--wrap=none'), outputfile=txtFilename)\n",
        "                assert output == \"\"\n",
        "                self.files_ext = 'txt'\n",
        "            return\n",
        "            # return self.txt_files\n",
        "\n",
        "\n",
        "    def clean_split_txt(self, file_path):\n",
        "        # Delete junk from a txt file\n",
        "        counter = 0\n",
        "\n",
        "        if not os.path.exists(f\"{data_path}/Datasets/{data_to_conv}\"):\n",
        "            os.makedirs(f\"{data_path}/Datasets/{data_to_conv}\")\n",
        "\n",
        "        for infile in self.files: # , outfile in zip(self.files, self.txt_files):\n",
        "            # Read in the file\n",
        "            with open(infile, 'r') as file:\n",
        "                filedata = file.read()\n",
        "            # Replace the target string\n",
        "            for word in self.delete_list:\n",
        "                filedata = filedata.replace(word, \"\")\n",
        "            # Write the file out again\n",
        "            with open('temp.txt', 'w') as file:\n",
        "                file.write(filedata)\n",
        "            with open('temp.txt', 'r') as f:\n",
        "              lines = f.readlines()\n",
        "              print(lines[50:80])\n",
        "\n",
        "            with open('temp.txt', 'w') as f:\n",
        "              for line in lines:\n",
        "                if ('N' not in line) and ('КонсультантПлюс' not in line) and (not line.isspace()) and (not any(word.isupper() for word in line.split()) or sum(1 for c in line if c.isupper()) <= 2):\n",
        "                  f.write(line)\n",
        "\n",
        "              # for line in lines:\n",
        "              #     filedata = '\\n'.join(('N' not in line) and ('КонсультантПлюс' not in line)\n",
        "              #                           and (not line.isspace())\n",
        "              #                           and (not any(word.isupper() for word in line.split())\n",
        "              #                             or sum(1 for c in line if c.isupper()) <= 2))\n",
        "\n",
        "            # filtered_lines = [line for line in lines if 'N' not in line and 'КонсультантПлюс' not in line\n",
        "            #                   and not line.isspace() and (not any(word.isupper() for word in line.split())\n",
        "            #                   or sum(1 for c in line if c.isupper()) <= 2)]\n",
        "\n",
        "            # filedata = '\\n'.join(filtered_lines)\n",
        "\n",
        "            with open('temp.txt') as f:\n",
        "                filedata = f.read()\n",
        "\n",
        "            also_delete = {':\\n': ': ', ': \\n': ': ',\n",
        "                            ';\\n': '; ', '; \\n': '; '}\n",
        "            for key, value in also_delete.items():\n",
        "                filedata = filedata.replace(key, value)\n",
        "            paragraphs = filedata.split('\\n')\n",
        "\n",
        "            for paragraph in paragraphs:\n",
        "                filename = f\"{data_path}/Datasets/{data_to_conv}/{str(counter).zfill(6)}.txt\"\n",
        "                counter += 1\n",
        "                with open(filename, 'w') as file:\n",
        "                    file.write(paragraph)\n",
        "        return\n",
        "\n",
        "\n",
        "    def clean_split_csv(self, file_path):\n",
        "        counter = 0\n",
        "        for infile in self.files:\n",
        "            # Open the CSV file and read\n",
        "            with open(infile, 'r') as filedata:\n",
        "                reader = csv.DictReader(filedata, quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "                if not os.path.exists(f\"{data_path}/Datasets/{data_to_conv}\"):\n",
        "                    os.makedirs(f\"{data_path}/Datasets/{data_to_conv}\")\n",
        "                    print('new directory made')\n",
        "                # Iterate through the rows and apply the replacement to the 'text' column\n",
        "                for row in reader:\n",
        "                    paragraph = row['Текст'].replace('\\n', ' ')\n",
        "                    # print(text)\n",
        "                    filename = f\"{data_path}/Datasets/{data_to_conv}/{str(counter).zfill(6)}.txt\"\n",
        "                    counter += 1\n",
        "                    with open(filename, 'w') as file:\n",
        "                        file.write(paragraph)\n",
        "        return\n",
        "\n",
        "\n",
        "    def make_to_dataset(self, files):\n",
        "        if not os.path.exists(f\"{data_path}/Datasets/{data_to_conv}\"):\n",
        "            os.makedirs(f\"{data_path}/Datasets/{data_to_conv}\")\n",
        "        else:\n",
        "            print('If errors, try refreshing the directory or this notebook manually.')\n",
        "\n",
        "\n",
        "        for file_path in [os.path.join(files, file) for file in os.listdir(files)]:\n",
        "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
        "                text = infile.read()\n",
        "            # Tokenize the text into words\n",
        "            words = word_tokenize(text)\n",
        "\n",
        "            tagged_words = []\n",
        "            prev_word = None  # Keep track of the previous word\n",
        "            for word in words:\n",
        "                if word in self.punct_marks:\n",
        "                    if prev_word is not None:\n",
        "                        tagged_words.append((prev_word, self.punct_marks[word]))\n",
        "                    prev_word = None  # Reset prev_word after tagging it\n",
        "                else:\n",
        "                    if prev_word is not None:\n",
        "                        tagged_words.append((prev_word, 'O'))\n",
        "                    prev_word = word  # Update prev_word for the next iteration\n",
        "            # Write the tagged words to the text file\n",
        "            with open(file_path, 'w', encoding='utf-8') as outfile:\n",
        "                for tagged_word in tagged_words:\n",
        "                    outfile.write(tagged_word[0] + '\\t' + tagged_word[1] + '\\n')\n"
      ],
      "metadata": {
        "id": "bu6jLmzAEKeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_to_conv_path = f'{data_path}/Raw_data/{data_to_conv}'\n",
        "debug_prep = Files_Prep(data_to_conv, files_ext)\n",
        "debug_prep.process_files()\n"
      ],
      "metadata": {
        "id": "wg0vOms6FtAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_path = '/content/drive/MyDrive/Data_punct'\n",
        "# print(os.listdir(f'{data_path}/Datasets/{data_to_conv}'))"
      ],
      "metadata": {
        "id": "nPzaE9ca8wGh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}